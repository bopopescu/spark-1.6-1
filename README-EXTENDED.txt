
---------------------------------------------
Author: Omid Mashayekhi <omidm@stanford.edu>
---------------------------------------------

I have collected concise directives on how to install and build Spark, as well
as launching a standalone spark cluster and submitting applications for
execution to the cluster. There is also information about activating and
deactivating the logs and monitoring the progress. Note that deactivating the
logs may speed things up drasticaly. You can also find directives on dumping
the assembly code generated by JIT at the workers, as well as decompiling the
bite code (*.class files) to get the java file. 

Also I have added extra examples and README in the "extended" folder, that you
can take a look at. There are a couple of main documentation pages that I have
copied in "extended/docs" for reference. For comprehensive documentation refer
to: http://spark.apache.org/documentation.html (Spark 1.6.0)


-------------------------------------------------------------------------------
How to get, compile, and run examples
-------------------------------------------------------------------------------

Please refer to: extended/docs/[overview, building].html

1. Download the spark source code from http://spark.apache.org/downloads.html


2. Build using following command:

    $ build/mvn -DskipTests clean package

   Note: You may need to install Java (7+) first; for Ubuntu 16.04:

    $ sudo apt-get install openjdk-8-jdk
    $ sudo apt-get install openjdk-8-jre

   On Ubuntu 12.04, you need the older versions:

    $ sudo apt-get install openjdk-7-jdk
    $ sudo apt-get install openjdk-7-jre


3. Run one of the Java or Scala examples in examples/src/main use the
   "bin/run-example" script that invoces general "spark-submit" script:

    $ ./bin/run-example <example-class>

   For example, to run logistic regression app at:
      examples/src/main/scala/org/apache/spark/examples/LocalLR.scala
      examples/src/main/scala/org/apache/spark/examples/SparkLR.scala

    $ ./bin/run-example LocalLR
    $ ./bin/run-example SparkLR

  You can edit the files and rebuild, as well:
  (inceremntal build seems not working)

    $ build/mvn -DskipTests package


-------------------------------------------------------------------------------
How to launch a standalone cluster and run an applications 
-------------------------------------------------------------------------------

Please refer to: extended/docs/[cluster, submitting, config].html 

1. Launch the master and slave instances by running the following scripts on the
   machines that you want to run each instace on:
   
    $ ./sbin/start-master.sh
    $ ./sbin/start-slave.sh spark://<master-url>:7077

  NOTE: you need the master URL (e.g. kitkat) and NOT the ip address.

  NOTE: The slave script detects the number of cores and memory on the machine,
  or you can set the resources using the available options as follow:

    -h HOST, --host HOST  Hostname to listen on

    -p PORT, --port PORT  Port for service to listen on
                          (default: 7077 for master, random for worker)

    --webui-port PORT  Port for web UI
                       (default: 8080 for master, 8081 forworker)

    -c CORES, --cores CORES   Total CPU cores to allow Spark applications to
                              use on the machine (default: all available);
                              only on worker

    -m MEM, --memory MEM  Total amount of memory to allow Spark applications to
                          use on the machine, in a format like 1000M or 2G
                          (default: your machine's total RAM minus 1 GB);
                          only on worker

    -d DIR, --work-dir DIR  Directory to use for scratch space and job output
                            logs (default: SPARK_HOME/work); only on worker

    --properties-file FILE  Path to a custom Spark properties file to load
                            (default: conf/spark-defaults.conf)


2. create an application bundle in form of jar file. Please refer to the
   following section for more info or look at the logistic-regression example
   in the "extended" folder.


3. Submitt the application for execution usising the "spark-submit" script on
   the master machine in the "client" mode:

    $ ./bin/spark-submit
          --class <main-class>
          --master spark://<master-url>:7077
          --deploy-mode client
          <application-jar> [application-arguments]

  NOTE: you need the master URL (e.g. kitkat) and NOT the ip address.

  NOTE: one useful option is "--executor-memory", which specifies the available
  memory per task (the default value is 1G but for some tasks it is not enough).
  note that it is different than "--memory" option for worker, which specifies
  the total available memory at worker. for other options and variants please
  see extended/docs/[submitting, config].html, or run spark-submit with the
  --help option.

  For example to run:
    examples/src/main/scala/org/apache/spark/examples/SparkLR.scala
  after launching master and slave:

    $ ./bin/spark-submit
        --class org.apache.spark.examples.SparkLR
        --master spark://<master-url>:7077
        examples/target/spark-examples_2.10-1.6.0.jar


4. Stop the master and slave instances by running the following scripts on the
   machine that hosts each instace:
   
    $ ./sbin/stop-master.sh
    $ ./sbin/stop-slave.sh



-------------------------------------------------------------------------------
How to create an application bundle
-------------------------------------------------------------------------------

Please refer to: extended/docs/quick.html


1. Install sbt, by following the instructions at:
    http://www.scala-sbt.org/0.13/docs/Installing-sbt-on-Linux.html

    $ echo "deb https://dl.bintray.com/sbt/debian /" | sudo tee -a /etc/apt/sources.list.d/sbt.list
    $ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 642AC823
    $ sudo apt-get update
    $ sudo apt-get install sbt


2. Create an sbt file with following content:

    - name := "Simple Project"
    - version := "1.0"
    - scalaVersion := "2.10.5"
    - libraryDependencies += "org.apache.spark" %% "spark-core" % "1.6.0"


3. Your directory layout should look like this:

    $ find .
    > ./simple.sbt
    > ./src
    > ./src/main
    > ./src/main/scala 
    > ./src/main/scala/SimpleApp.scala


4. create the jar file by issuing sbt in the folder:

    $ sbt package


5. For an example please check "extended/logistic-regression"


** NOTE **
you can also use "scalac" and "jar" directly to make the application bundle
form the source file. This does not require the weird directory structure and
sbt installation. Note that for spark 1.6.0 you need to use scala version 2.10;
as part of installing spark, the correct version of scala/scalac is also
installed in the "build/" folder, and you should use that for this matter. Also
you need to link againt spark library in jar format, which is also built during
the spark installation.

    $ <SPARK_HOME>/build/scala-2.10.5/bin/scalac
                  -cp <SPARK_HOME>//assembly/target/scala-2.10/spark-assembly-1.6.0-hadoop2.2.0.jar
                  <application>.scala
    $ jar cf <appication>.jar *.class



-------------------------------------------------------------------------------
How to monitor and activate/deactivate loggings
-------------------------------------------------------------------------------

Please refer to: extended/docs/[monitoring, config].html

1. To monitor the application progress connect to the master webUI:
    
    http://<master-url>:8080


2. To activate event logging for the application, first make a directory to
   hold the logs, then pass following extra options to "spark-submit":

    $ mkdir log-dir
    $ ./bin/spark-submit
        --conf spark.eventLog.enabled=true
        --conf spark.eventLog.dir=<log-dir>
        ... the rest as before ...

    The logs are in JASON format. To view the logs connect to  master webUI,
    and browse the link named after application (e.g. SparkLR). Also, you can
    open the JASON file directly and read in data for more precice statistics.
    There are loggings for various events in the lifetime of the application
    with timestamps. For example each iteration of the LR application has
    following events:

      SparkListenerJobStart
      SparkListenerStageSubmitted

      <pairs-of-start-end-per-task-id> 
      SparkListenerTaskStart
      SparkListenerTaskEnd

      SparkListenerStageCompleted
      SparkListenerJobEnd

    You can get precise task duration and scheduling overhead information from
    the event timestamps in milliseconds.  There are python scripts in the
    "extended/" folder I have written to get some of that information.


3. To deactivate all Spark internal logging so that it runs faster: 

    $ cp conf/log4j.properties.template conf/log4j.properties

   Edit conf/log4j.properties:
        replace "log4j.rootCategory=INFO, console" by "log4j.rootCategory=WARN, console"



-------------------------------------------------------------------------------
How to get the assembly code generated by JIT at workers.
-------------------------------------------------------------------------------

Please refer to: extended/docs/config.html

1. Put external library "libhsdis-amd64.so" in the following java path:

    $ cp extended/extern/libhsdis-amd64.so  $JAVA_HOME/lib/amd64/server

   If JAVA_HOME is not already set, it is probably something like:

    /usr/lib/jvm/java-7-openjdk-amd64/jre/

   Make sure that the java version is the same as the one Spark uses.
   For more info on getting the library go to:

      http://mechanical-sympathy.blogspot.com/2013/06/printing-generated-assembly-code-from.html


2. Add the extra java options through one of the following mechanisms:

   A. "***DEPRECATED***" If conf/spark-env.sh does not already exist, first
   copy it from the template file: 

    $ cp conf/spark-env.sh.template conf/spark-env.sh 

   Add the following line to conf/spark-env.sh:

      export SPARK_JAVA_OPTS="-XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly"


   B. If conf/spark-defaults.conf does not already exist, first copy it from the
   template file: 

    $ cp conf/spark-defaults.conf.template conf/spark-defaults.conf 

   Add the following line to conf/spark-defaults.conf:

      spark.driver.extraJavaOptions  -XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly
      spark.executor.extraJavaOptions  -XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly


3. Run the application as explained in the previous section. The dumped
   assembly is in "stdout" of slave found at "work/" directory by default
   (can be changed through -d option of the "spark-submit"). You can also view
   the file through the webUI.



-------------------------------------------------------------------------------
How to control JIT compiler
-------------------------------------------------------------------------------

If conf/spark-defaults.conf does not already exist, first copy it from the template file: 

    $ cp conf/spark-defaults.conf.template conf/spark-defaults.conf 

Add the following line to conf/spark-defaults.conf:

   spark.driver.extraJavaOptions  -XX:CompileThreshold=0 (or 1 or 10000)
   spark.executor.extraJavaOptions  -XX:CompileThreshold=0 (or 1 or 10000)


*** Another useful option: -XX:+PrintCompilation

*** How to get os counters: perf stat -B (java/scala) ... program args


-------------------------------------------------------------------------------
How to get Java impelmentation of scala objects
-------------------------------------------------------------------------------

First decompress the the scala jar library:

    $ jar xf build/scala-2.10.5/lib/scala-library.jar

then go for example to scala/collection/mutable/ to get the .class files, and
decompile with javap

    $ javap <class-name>.class

or you can decompile the .class files with ./jd-gui (details are below)



-------------------------------------------------------------------------------
How to decompile the bytecode (.jar/.class file) to get the java file.
-------------------------------------------------------------------------------
  
1. Download jd-gui from http://jd.benow.ca/

2. it is compile for 32bit, so for 64bit machines you need to install the following packages:
    $ sudo apt-get install libgmp3c2
    $ sudo apt-get install libgtk2.0-0:i386
    $ sudo apt-get install libgtk2.0-0
    $ sudo apt-get install ia32-libs-gtk

3. untar the package and run ./jd-gui. open the target .jar file through the gui.



NOTE: you can run javap to disassemble the java class files and get the bytecode:
    $ javap -c  <class-file>


NOTE: if you already have the .scala code, use scalac with option -print
      for example while in extended/logistic-regression/gradient-scala

    $ ../../..//build/scala-2.10.5/bin/scalac Gradient.scala -print -d compiled/ 



-------------------------------------------------------------------------------
How to mange an EC2 cluster with provided scripts
-------------------------------------------------------------------------------

Please refer to: extended/docs/ec2.html 

Althoug you can manualy create AMI out of this directory after building the
spark, there are also scripts provided by developers that helps mange a cluster
of spark nodes on EC2. Following is a summary of the features.


1. To launch a master and a number of slave nodes:

    $ ./ec2/spark-ec2
        -k <key-name e.g. omidm-sing-key-pair-us-west-2>
        -i <key-file e.g. ~/.ssh/omidm-sing-key-pair-us-west-2.pem>
        -s <slave-num>
        launch <cluster-name>
        --region=<ec2-region e.g. us-west-2>
        --instance-type=<instance-type e.g. m1.large/c3.2xlarge>

2. To login to master instance:

    $ ./ec2/spark-ec2
        -k <key-name e.g. omidm-sing-key-pair-us-west-2>
        -i <key-file e.g. ~/.ssh/omidm-sing-key-pair-us-west-2.pem>
        login <cluster-name>
        --region=<e.g. us-west-2>

  NOTE: You could also directly ssh into instances, the username is "root":

    $ ssh -i <key-file> root@<ip-address>


3. To distribute a folder (e.g. new application bundle) over all nodes:

    $ ./spark-ec2/copy-dir <path/to/dir/dir-name> 











